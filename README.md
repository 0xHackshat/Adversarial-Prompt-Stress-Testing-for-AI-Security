# Adversarial-Prompt-Stress-Testing-for-AI-Security
The system generates adversarial prompts using an LLM, runs them against a target AI model  (cloud or local), captures the responses, and evaluates vulnerabilities. It helps identify weaknesses  like prompt injection, data leakage, or unsafe outputs. 
